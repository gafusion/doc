

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Computational Performance &mdash; GACODE 1.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="_static/contentui.css" />
      <link rel="stylesheet" type="text/css" href="_static/css/custom.css?v=a0fb6060" />

  
    <link rel="shortcut icon" href="_static/favicon.ico"/>
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="_static/documentation_options.js?v=2882ecd3"></script>
      <script src="_static/doctools.js?v=9bcbadda"></script>
      <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="_static/contentui.js"></script>
      <script>window.MathJax = {"tex": {"macros": {"qfilt": "{\\mathcal{Q}}", "exb": "{\\mathbf{E} \\hskip -1pt \\times \\hskip -1pt \\mathbf{B}}", "df": "{\\delta \\hskip -0.5pt f}", "vpar": "{{\\rm v}_\\parallel}", "vta": "{{\\rm v}_{ta}}", "bhat": "{\\textbf{b}}", "dv": "{d^{\\hskip 1pt 3} \\hskip -1pt {\\rm v}}", "jp": "{{\\cal J}_\\psi}", "B": "{\\mathbf{B}}", "bunit": "{B_\\mathrm{unit}}", "betae": "{\\beta_{e,{\\rm unit}}}", "dphi": "{\\delta \\hskip -1pt \\phi}", "dap": "{\\delta \\hskip -1pt A_\\parallel}", "dbp": "{\\delta \\hskip -1pt B_\\parallel}", "dphif": "{\\delta \\hskip -0.5pt \\widetilde{\\phi}}", "dapf": "{\\delta \\hskip -0.5pt \\widetilde{A}_\\parallel}", "dbpf": "{\\delta \\hskip -0.5pt \\widetilde{B}_\\parallel}", "na": "{N_\\alpha}", "nr": "{N_r}", "nx": "{N_\\xi}", "nux": "{N_u}", "nt": "{N_\\theta}", "ux": "{u}", "uxa": "{{\\ux}_a}", "uxb": "{{\\ux}_b}", "kpv": "{{\\bf k}_\\perp}", "md": "{m_\\mathrm{D}}", "Hf": "{\\widetilde{H}_a}", "Hk": "{H_{a,\\kpv}}", "Hkb": "{H_{b,\\kpv}}", "Hfb": "{\\widetilde{H}_b}", "pf": "{\\widetilde{\\Psi}_a}", "rhos": "{\\rho_s}", "hf": "{\\widetilde{h}_a}", "kperp": "{{\\mathbf k}_\\perp}", "qgb": "{Q_\\mathrm{GB}}", "kx": "{k_x^0}", "fwid": "{3.2in}", "nlc": "{\\texttt{nl03}\\xspace}", "rhof": "{\\rho_\\mathrm{f}}"}}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="CGYRO Workshop 2022" href="workshop.html" />
    <link rel="prev" title="input.gacode" href="input_gacode.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            GACODE
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Installation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="README.html">Using GACODE</a></li>
<li class="toctree-l1"><a class="reference internal" href="setup.html">Prebuilt Environments</a></li>
<li class="toctree-l1"><a class="reference internal" href="build.html">Build on your laptop</a></li>
<li class="toctree-l1"><a class="reference internal" href="zreferences.html">References</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Codes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cgyro.html">CGYRO</a></li>
<li class="toctree-l1"><a class="reference internal" href="neo.html">NEO</a></li>
<li class="toctree-l1"><a class="reference internal" href="tgyro.html">TGYRO</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Legacy tools</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="gyro.html">GYRO</a></li>
<li class="toctree-l1"><a class="reference internal" href="tglf.html">TGLF</a></li>
<li class="toctree-l1"><a class="reference internal" href="prgen.html">profiles_gen</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Physics</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="geometry.html">Flux-Surface Geometry</a></li>
<li class="toctree-l1"><a class="reference internal" href="rotation.html">Rotation Theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="input_gacode.html">input.gacode</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Misc</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Computational Performance</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#computational-approach">Computational Approach</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#library-overview">Library overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="#gyrokinetic-framework">Gyrokinetic framework</a></li>
<li class="toctree-l3"><a class="reference internal" href="#time-advance">Time advance</a></li>
<li class="toctree-l3"><a class="reference internal" href="#fft-based-evaluation-of-the-nonlinearity">FFT-based evaluation of the nonlinearity</a></li>
<li class="toctree-l3"><a class="reference internal" href="#array-layouts-and-communication">Array layouts and communication</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#parallel-performance-and-scalability">Parallel Performance and Scalability</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#strong-scaling-performance">Strong-scaling performance</a></li>
<li class="toctree-l3"><a class="reference internal" href="#kernel-based-performance-analysis">Kernel-based performance analysis</a></li>
<li class="toctree-l3"><a class="reference internal" href="#parallel-i-o-performance">Parallel I/O performance</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#porting-and-optimizing-for-olcf-frontier">Porting and Optimizing for OLCF Frontier</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="workshop.html">CGYRO Workshop 2022</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">GACODE</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Computational Performance</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
  .ss-layout-default-AB { grid-template-areas: 'A B'; }
  .ss-layout-default-A { grid-template-areas: 'A'; }
</style>
<section id="computational-performance">
<h1>Computational Performance<a class="headerlink" href="#computational-performance" title="Link to this heading"></a></h1>
<p><strong>Updated April 2025</strong></p>
<p>Recent information about the computational approach and scaling performance of CGYRO <span id="id1">[<a class="reference internal" href="zreferences.html#id193" title="J. Candy, E.A. Belli, and R.V. Bravenec. A high-accuracy Eulerian gyrokinetic solver for collisional plasmas. J. Comput. Phys., 324:73, 2016. doi:10.1016/j.jcp.2016.07.039.">CBB16</a>, <a class="reference internal" href="zreferences.html#id195" title="J. Candy, I. Sfiligoi, E. Belli, K. Hallatschek, C. Holland, N. Howard, and E.D`Azevedo. Multiscale-optimized plasma turbulence simulation on petascale architechtures. Computers &amp; Fluids, 188:125, 2019. doi:10.1016/j.compfluid.2019.04.016.">CSB+19</a>, <a class="reference internal" href="zreferences.html#id968" title="I. Sfiligoi, E. Belli, J. Candy, and R.D. Budiardja. Optimization and portability of a fusion OpenACC-based HPC code from NVIDIA to AMD GPUs. In PEARC '23: Practice and Experience in Advanced Research Computing, Association for Computing Machinery, pages 246-250, 2023.">SBCB23</a>, <a class="reference internal" href="zreferences.html#id967" title="I. Sfiligoi, E. Belli, J. Candy, and F. Wurthwein. Comparing single-node and multi-node performance of an important fusion HPC code benchmark. In PEARC '22: Practice and Experience in Advanced Research Computing, Association for Computing Machinery, Article No. 10, p.1-4, 2022.">SBCW22</a>]</span> is documented here.</p>
<section id="computational-approach">
<h2>Computational Approach<a class="headerlink" href="#computational-approach" title="Link to this heading"></a></h2>
<section id="library-overview">
<h3>Library overview<a class="headerlink" href="#library-overview" title="Link to this heading"></a></h3>
<p>CGYRO is written in Fortran, with GPU accelerator optimizations implemented using OpenACC or OpenMP offloading, and communication with GPU-aware MPI. Heavy use of cuFFT/rocFFT is made for the nonlinear term. Analysis tools are based on Python scripts. Exceptional GPU scaling performance on OLCF Summit has been demonstrated through previous ALCC and INCITE Awards over the years 2019-2024. On Summit, the following modules were used: <code class="docutils literal notranslate"><span class="pre">pgi,</span> <span class="pre">spectrum-mpi,</span> <span class="pre">fftw,</span> <span class="pre">netlib-lapack,</span> <span class="pre">essl,</span> <span class="pre">cuda,</span> <span class="pre">python.</span></code>  In early 2023, CGYRO was ported to OLCF Frontier <span id="id2">[<a class="reference internal" href="zreferences.html#id968" title="I. Sfiligoi, E. Belli, J. Candy, and R.D. Budiardja. Optimization and portability of a fusion OpenACC-based HPC code from NVIDIA to AMD GPUs. In PEARC '23: Practice and Experience in Advanced Research Computing, Association for Computing Machinery, pages 246-250, 2023.">SBCB23</a>]</span>, and capability-scale simulations using more than 20% of the system were carried out in a 2023-2024 INCITE project. Over 87% of CGYRO usage for our 2024 INCITE was capability-scale (over 90% for 2023 INCITE), using at least 2048 nodes (8192 MI250X accelerators) on Frontier. On Frontier, CGYRO uses the HPE Cray Compiling Environment (CCE) Fortran compiler <code class="docutils literal notranslate"><span class="pre">(PrgEnv-cray,</span> <span class="pre">cray-mpich)</span></code>, <code class="docutils literal notranslate"><span class="pre">craype-accel-amd-gfx90a</span></code> (for GPU-aware Cray mpich), <code class="docutils literal notranslate"><span class="pre">cray-python,</span> <span class="pre">rocm</span></code> and AMD <code class="docutils literal notranslate"><span class="pre">hipfort</span></code> providing Fortran interfaces for calling HIP libraries <code class="docutils literal notranslate"><span class="pre">hipFFT/rocFFT</span></code>.</p>
</section>
<section id="gyrokinetic-framework">
<h3>Gyrokinetic framework<a class="headerlink" href="#gyrokinetic-framework" title="Link to this heading"></a></h3>
<p>CGYRO solves the nonlinear, electromagnetic gyrokinetic equations  <span id="id3">[<a class="reference internal" href="zreferences.html#id1027" title="H. Sugama and W. Horton. Nonlinear electromagnetic gyrokinetic equation for plasmas with large mean flows. Phys. Plasmas, 5:2560, 1998. doi:10.1063/1.872941.">SH98</a>]</span> for 5D particle distributions <span class="math notranslate nohighlight">\(\Hf(k_x,k_y,\theta;\xi,{\rm v})\)</span>, where the subscript <span class="math notranslate nohighlight">\(a\)</span> is the species index, and tildes indicate a Fourier-space quantity. The <em>spatial coordinates</em> are</p>
<div class="math notranslate nohighlight" id="equation-eq-coord">
<span class="eqno">(2)<a class="headerlink" href="#equation-eq-coord" title="Link to this equation"></a></span>\[\begin{split}\begin{align}
k_x    &amp; \rightarrow  \text{radial wavenumber}   \; ,  \\
k_y    &amp; \rightarrow  \text{binormal wavenumber} \; ,  \\
\theta &amp; \rightarrow  \text{field-line coordinate}
\end{align}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(k_\perp^2 = k_x^2 + k_y^2\)</span>, and the <em>velocity-space</em> coordinates are</p>
<div class="math notranslate nohighlight">
\[\begin{split}\xi = \vpar/{\rm v}  &amp; \rightarrow  \text{pitch angle cosine} \in [-1,1] \; , \\
{\rm v}              &amp; \rightarrow  \text{speed} \in [0,\infty] \; .\end{split}\]</div>
<p>The use of <em>twisted fieldline coordinates</em> gives radial wavenumbers <span class="math notranslate nohighlight">\(k_x\)</span> that depend on <span class="math notranslate nohighlight">\(\theta\)</span> and <span class="math notranslate nohighlight">\(k_y\)</span> <span id="id4">[<a class="reference internal" href="zreferences.html#id193" title="J. Candy, E.A. Belli, and R.V. Bravenec. A high-accuracy Eulerian gyrokinetic solver for collisional plasmas. J. Comput. Phys., 324:73, 2016. doi:10.1016/j.jcp.2016.07.039.">CBB16</a>]</span>. For this reason, we define a simple radial wavenumber <span class="math notranslate nohighlight">\(\kx\)</span> (the value of <span class="math notranslate nohighlight">\(k_x\)</span> at <span class="math notranslate nohighlight">\(\theta=0\)</span>). The <span class="math notranslate nohighlight">\((k_x,k_y)\)</span> spectral representation facilitates the arbitrary wavelength formulation by diagonalizing the gyroradius operator. The GK equations are written in terms of an EM <em>field potential</em> <span class="math notranslate nohighlight">\(\pf\)</span>,</p>
<div class="math notranslate nohighlight" id="equation-eq-fpto">
<span class="eqno">(3)<a class="headerlink" href="#equation-eq-fpto" title="Link to this equation"></a></span>\[ \pf = J_{0}(k_\perp \rho_a) \left( \dphif - \frac{\vpar}{c} \dapf \right) + \frac{m_a {\rm v}_{\perp}^2}{z_{a} e B} \frac{J_{1}(k_\perp \rho_a)}{k_\perp \rho_a} \dbpf \; ,\]</div>
<p>where <span class="math notranslate nohighlight">\(m_a, z_a\)</span> and <span class="math notranslate nohighlight">\(\rho_a\)</span> are the species mass, charge and gyroradius.  Above, <span class="math notranslate nohighlight">\((\dphif,\dapf,\dbpf)\)</span> are the electrostatic and transverse/compressional EM potentials, respectively, computed from the gyrokinetic Maxwell equations <span id="id5">[<a class="reference internal" href="zreferences.html#id1027" title="H. Sugama and W. Horton. Nonlinear electromagnetic gyrokinetic equation for plasmas with large mean flows. Phys. Plasmas, 5:2560, 1998. doi:10.1063/1.872941.">SH98</a>]</span>.  The Bessel functions <span class="math notranslate nohighlight">\(J_{0}\)</span> and <span class="math notranslate nohighlight">\(J_{1}\)</span> arise from gyroaveraging. In terms of <span class="math notranslate nohighlight">\(\pf\)</span>, the GK equation for species <span class="math notranslate nohighlight">\(a\)</span> can be written as</p>
<div class="math notranslate nohighlight" id="equation-eq-gk">
<span class="eqno">(4)<a class="headerlink" href="#equation-eq-gk" title="Link to this equation"></a></span>\[\frac{\partial \hf}{\partial \tau} + A(\Hf,\pf) + B(\Hf,\pf) = 0 \; ,\]</div>
<p>with <span class="math notranslate nohighlight">\(\tau \doteq (c_s/a) t\)</span> the normalized time, <span class="math notranslate nohighlight">\(a\)</span> the separatrix minor radius, <span class="math notranslate nohighlight">\(c_s=\sqrt{T_e/m_D}\)</span> the deuteron (mass <span class="math notranslate nohighlight">\(m_D\)</span>) sound speed, and <span class="math notranslate nohighlight">\(T_e\)</span> the electron temperature. <span class="math notranslate nohighlight">\(A(\Hf,\pf)\)</span> and <span class="math notranslate nohighlight">\(B(\Hf,\pf)\)</span> represent the <strong>collisionless</strong> and <strong>collisional</strong> terms, respectively. In Eq. <a class="reference internal" href="#equation-eq-gk">(4)</a>, <span class="math notranslate nohighlight">\(\Hf\)</span> is the <em>nonadiabatic distribution</em> whereas <span class="math notranslate nohighlight">\(\hf\)</span> is a <em>modified distribution</em> suitable for numerical time-integration.  They are related through the field potential by <span class="math notranslate nohighlight">\(\hf = \Hf - (z_a T_e)/T_a \pf\)</span>.</p>
<div class="math notranslate nohighlight" id="equation-eq-maxwell">
<span class="eqno">(5)<a class="headerlink" href="#equation-eq-maxwell" title="Link to this equation"></a></span>\[\begin{split}\left( k_\perp^2 \lambda_D^2 n_e + \sum_a \frac{z_a^2 T_e}{T_a} n_a \right) \dphif &amp; = \sum_a z_a e \int \hskip -3pt \dv f_{0a} J_0(k_\perp \rho_a) \Hf \; , \\
- \frac{2 n_e}{\betae} k_\perp^2 \rho_s^2 \dapf &amp; = \sum_a z_a e \int \hskip -3pt \dv \frac{\vpar}{c_s} f_{0a} J_0(k_\perp \rho_a) \Hf \; \\
  -\frac{2 n_e}{\betae} \frac{B}{B_{\rm unit}} \dbpf &amp; = \sum_a \int \hskip -3pt \dv \frac{m_a {\rm v}_{\perp}^2}{T_e} f_{0a} \frac{J_1(k_\perp \rho_a)}{k_\perp \rho_a} \Hf \; .\end{split}\]</div>
<p>Here <span class="math notranslate nohighlight">\(\lambda_D = \sqrt{T_e/(4 \pi n_e e^2)}\)</span> is the Debye length and <span class="math notranslate nohighlight">\(\betae = 8 \pi n_e T_e/\bunit^2\)</span> is the effective electron beta, where <span class="math notranslate nohighlight">\(\bunit\)</span> is the effective magnetic field <span id="id6">[<a class="reference internal" href="zreferences.html#id189" title="J. Candy and E. Belli. GYRO Technical Guide. General Atomics Technical Report, 2010.">CB10</a>]</span>.</p>
</section>
<section id="time-advance">
<h3>Time advance<a class="headerlink" href="#time-advance" title="Link to this heading"></a></h3>
<p>Operator splitting is used to separate the evolution of <span class="math notranslate nohighlight">\(A(\Hf,\pf)\)</span> and <span class="math notranslate nohighlight">\(B(\Hf,\pf)\)</span>. This allows the nonlinear, collisionless dynamics <span class="math notranslate nohighlight">\((A)\)</span> to be treated explicitly, while the collisional dynamics <span class="math notranslate nohighlight">\((B)\)</span> is advanced implicitly.  First, the <strong>collisionless step</strong> operates primarily on the spatial dimensions and is distributed in the velocity dimensions, requiring solution of</p>
<div class="math notranslate nohighlight" id="equation-eq-gk-a">
<span class="eqno">(6)<a class="headerlink" href="#equation-eq-gk-a" title="Link to this equation"></a></span>\[\frac{\partial \hf}{\partial \tau} + A(\Hf,\pf) = 0 \; .\]</div>
<p>We write the collisionless term as:</p>
<div class="math notranslate nohighlight" id="equation-eq-nl">
<span class="eqno">(7)<a class="headerlink" href="#equation-eq-nl" title="Link to this equation"></a></span>\[\begin{split}\begin{multline}
A(\Hf,\pf) = -i \left( \Omega_{\rm parallel} + \Omega_{\rm drift} \right) \Hf
- i \Omega_* \pf -i \Omega_s X \hf \\
  \qquad - \frac{c}{B} \frac{a}{c_s} \sum_{\kpv^\prime + \kpv^{\prime \prime} = \kpv}
    \left( \bhat \cdot \kpv^\prime \times \kpv^{\prime \prime} \right)
    \pf(\kpv^\prime) \hf(\kpv^{\prime \prime}) \; .
\end{multline}\end{split}\]</div>
<p>The linear terms in <span class="math notranslate nohighlight">\(A\)</span> include the parallel streaming along the field line <span class="math notranslate nohighlight">\(\Omega_{\rm parallel}\)</span>, drift motion perpendicular to the field line <span class="math notranslate nohighlight">\(\Omega_{\rm drift}\)</span>, <em>instability drive</em> from equilibrium density and temperature gradients <span class="math notranslate nohighlight">\(\Omega_*\)</span>, and <span class="math notranslate nohighlight">\(\exb\)</span> shear (see Ref. <span id="id7">[<a class="reference internal" href="zreferences.html#id193" title="J. Candy, E.A. Belli, and R.V. Bravenec. A high-accuracy Eulerian gyrokinetic solver for collisional plasmas. J. Comput. Phys., 324:73, 2016. doi:10.1016/j.jcp.2016.07.039.">CBB16</a>]</span>). These linear terms define the <em>streaming kernel</em>, hereafter referred to as <code class="docutils literal notranslate"><span class="pre">str</span></code>. The last term in Eq. <a class="reference internal" href="#equation-eq-nl">(7)</a> is a convolution (Poisson bracket in real space).  This defines the <em>nonlinear kernel</em> and is hereafter referred to as <code class="docutils literal notranslate"><span class="pre">nl</span></code>. Note that <strong>global capability</strong> (profile-curvature, or radial profile variation of the plasma density, temperature, and rotation of the equilibrium state) is enabled using a novel <em>wavenumber advection</em> scheme <span id="id8">[<a class="reference internal" href="zreferences.html#id194" title="J. Candy and E.A. Belli. Spectral treatment of gyrokinetic shear flow. J. Comput. Phys., 356:448, 2018. doi:10.1016/j.jcp.2017.12.020.">CB18</a>, <a class="reference internal" href="zreferences.html#id196" title="J. Candy, E.A. Belli, and G. Staebler. Spectral treatment of gyrokinetic profile curvature. Plasma Phys. Control. Fusion, 62:042001, 2020. doi:10.1088/1361-6587/ab759c.">CBS20</a>]</span>.  Explicit coupling with the Maxwell equations is also required to advance <span class="math notranslate nohighlight">\(\pf\)</span> in time. This operation defines the <em>field solve</em> kernel, hereafter referred to as <code class="docutils literal notranslate"><span class="pre">field</span></code>.  To advance Eq. <a class="reference internal" href="#equation-eq-gk-a">(6)</a>, RK4 or adaptive RK5(4)/RK7(6) methods are used.</p>
<p>The <strong>collisional step</strong> acts primarily on velocity dimensions and is distributed in the spatial dimensions:</p>
<div class="math notranslate nohighlight" id="equation-eq-cstep">
<span class="eqno">(8)<a class="headerlink" href="#equation-eq-cstep" title="Link to this equation"></a></span>\[\frac{\partial \hf}{\partial \tau} + B(\Hf,\pf) = 0 \quad \text{where} \quad
B(\Hf,\pf) = -i \Omega_{\xi} \Hf - \frac{a}{c_s} \sum_b C_{ab}^{L}(\Hf,\Hfb) \; .\]</div>
<p>Here <span class="math notranslate nohighlight">\(-i \Omega_{\xi}\)</span> describes linear trapped particle motion, and <span class="math notranslate nohighlight">\(C_{ab}^{L}\)</span> is the Sugama cross-species collision operator <span id="id9">[<a class="reference internal" href="zreferences.html#id1030" title="H. Sugama, T.-H. Watanabe, and M. Nunami. Linearized model collision operators for multiple ion species plasmas and gyrokinetic entropy balance equations. Phys. Plasmas, 16:112503, 2009. doi:10.1063/1.3257907.">SWN09</a>]</span>, which describes pitch-angle and energy diffusion. This is one of the most sophisticated collision operators available in numerical gyrokinetics.  A Legendre pseudospectral discretization in <span class="math notranslate nohighlight">\(\xi\)</span> is combined with a Steen pseudospectral discretization in <span class="math notranslate nohighlight">\({\rm v}\)</span>. Using a weak form of the discrete collision matrix, we construct a self-adjoint pseudospectral representation.  An implicit time-advance is necessary for stability without severe accuracy loss. Using a generalization of the Crank-Nicolson method, Eq. <a class="reference internal" href="#equation-eq-cstep">(8)</a> is advanced with a single matrix-vector multiply (matrix rank <span class="math notranslate nohighlight">\(N_\xi N_v N_a\)</span>).  The matrix is large and well-suited to execution on GPUs. The <em>collision kernel</em> is hereafter referred to as <code class="docutils literal notranslate"><span class="pre">coll</span></code>.</p>
</section>
<section id="fft-based-evaluation-of-the-nonlinearity">
<h3>FFT-based evaluation of the nonlinearity<a class="headerlink" href="#fft-based-evaluation-of-the-nonlinearity" title="Link to this heading"></a></h3>
<p>Evaulation of the quadratic nonlinearity in Eq. <a class="reference internal" href="#equation-eq-nl">(7)</a> is done in a standard way using a 2D dealiased FFT <span id="id10">[<a class="reference internal" href="zreferences.html#id806" title="S.A. Orszag. On the elimination of aliasing in finite-difference schemes by filtering high-wavenumber components. J. Atmos. Sci., 28:1074, 1971. doi:10.1175/1520-0469(1971)028&lt;1074:OTEOAI&gt;2.0.CO;2.">Ors71</a>]</span>.  To prevent <em>aliasing</em>, we zero-pad the spectral representation by a factor of <span class="math notranslate nohighlight">\(3/2\)</span>. The convolution conserves important flow invariants and eliminates a class of nonlinear instabilities from the numerical solution. To perform the forward and inverse FFTs, we use <code class="docutils literal notranslate"><span class="pre">FFTW</span></code> <span id="id11">[<a class="reference internal" href="zreferences.html#id371" title="M. Frigo and S.G. Johnson. The design and implementation of FFTW3. Proc. IEEE, 93 (2):216, 2005. doi:10.1109/JPROC.2004.840301.">FJ05</a>]</span> by default with options for cuFFT/rocFFT (GPU) on Summit/Frontier and Intel MKL on supported platforms.  First, we perform a series of four 2D complex-to-real <code class="docutils literal notranslate"><span class="pre">(c2r)</span></code> transforms:</p>
<div class="math notranslate nohighlight">
\[\begin{split}(i k_x) \pf &amp; \underset{\mathtt{c2r}}{\longrightarrow} \frac{\partial \Psi_a}{\partial x} \, , \\
(i k_x) \hf &amp; \underset{\mathtt{c2r}}{\longrightarrow} \frac{\partial h_a}{\partial x} \, , \\
(i k_y) \pf &amp; \underset{\mathtt{c2r}}{\longrightarrow} \frac{\partial \Psi_a}{\partial y} \, , \\
(i k_y) \hf &amp; \underset{\mathtt{c2r}}{\longrightarrow} \frac{\partial h_a}{\partial y} \; ,\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> are real-space meshpoints, such that all arrays are extended and
zero-padded by a factor of <span class="math notranslate nohighlight">\(3/2\)</span> (quantities without tildes are in real space).  The real-space
products are then taken, followed by the inverse transform of the entire nonlinearity via
a single 2D real-to-complex <code class="docutils literal notranslate"><span class="pre">r2c</span></code> transform</p>
<div class="math notranslate nohighlight">
\[- \frac{\partial \Psi_a}{\partial x} \frac{\partial h_a}{\partial y} +
 \frac{\partial h_a}{\partial x} \frac{\partial \Psi_a}{\partial y}
 \underset{\mathtt{r2c}}{\longrightarrow}  \left( \bhat \cdot
 \kpv^\prime \times \kpv^{\prime \prime} \right) \pf(\kpv^\prime)
 \hf(\kpv^{\prime \prime}) \; .\]</div>
<table class="docutils align-center" id="tab-kernels">
<caption><span class="caption-number">Table 49 </span><span class="caption-text">Summary of data properties of kernels. <code class="docutils literal notranslate"><span class="pre">str</span></code> refers to parallel streaming, <code class="docutils literal notranslate"><span class="pre">field</span></code> refers to the solution of the three Maxwell equations, <code class="docutils literal notranslate"><span class="pre">coll</span></code> refers to the implicit collision step, and <code class="docutils literal notranslate"><span class="pre">nl</span></code> refers to the nonlinear bracket (convolution).  In each case, the communication cost associated with each kernel is denoted by the <code class="docutils literal notranslate"><span class="pre">comm</span></code> suffix.</span><a class="headerlink" href="#tab-kernels" title="Link to this table"></a></caption>
<colgroup>
<col style="width: 25.0%" />
<col style="width: 50.0%" />
<col style="width: 25.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Kernel</p></th>
<th class="head"><p>Data dependence</p></th>
<th class="head"><p>Dominant operation</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">str</span></code></p></td>
<td><p><span class="math notranslate nohighlight">\({\kx,\theta},[k_y]_2,[\xi,{\rm v},a]_1\)</span></p></td>
<td><p>loop</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">field</span></code></p></td>
<td><p>Same as <code class="docutils literal notranslate"><span class="pre">str</span></code></p></td>
<td><p>loop</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">coll</span></code></p></td>
<td><p><span class="math notranslate nohighlight">\([\kx,\theta]_1,[k_y]_2,\xi,{\rm v},a\)</span></p></td>
<td><p>mat-vec multiply</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">nl</span></code></p></td>
<td><p><span class="math notranslate nohighlight">\({\kx,k_y},[\theta,[\xi,{\rm v},a]_1]_2\)</span></p></td>
<td><p>FFT</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">str_comm</span></code></p></td>
<td><p><span class="math notranslate nohighlight">\({\kx,\theta},[k_y]_2,\underline{[\xi,{\rm v},a]_1}\)</span></p></td>
<td><p>MPI_ALLREDUCE</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">field_comm</span></code></p></td>
<td><p>Same as <code class="docutils literal notranslate"><span class="pre">str_comm</span></code></p></td>
<td><p>MPI_ALLREDUCE</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">coll_comm</span></code></p></td>
<td><p><span class="math notranslate nohighlight">\(\kx,\theta,[k_y]_2,[\xi,{\rm v},a]_1\)</span><span class="math notranslate nohighlight">\(\leftrightarrow [\kx,\theta]_1,[k_y]_2,\xi,{\rm v},a\)</span></p></td>
<td><p>MPI_ALLTOALL</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">nl_comm</span></code></p></td>
<td><p><span class="math notranslate nohighlight">\(\kx,\theta,[k_y]_2,[\xi,{\rm v},a]_1\)</span><span class="math notranslate nohighlight">\(\leftrightarrow \kx,k_y,[\theta,[\xi,{\rm v},a]_1]_2\)</span></p></td>
<td><p>MPI_ALLTOALL</p></td>
</tr>
</tbody>
</table>
</section>
<section id="array-layouts-and-communication">
<h3>Array layouts and communication<a class="headerlink" href="#array-layouts-and-communication" title="Link to this heading"></a></h3>
<p>There are three computational array layouts.  Two are associated with the linear terms, and the third with the nonlinear kernel.  Internally, we define <em>lumped</em> variables for convenience; the configuration pair <span class="math notranslate nohighlight">\((\kx,\theta)\)</span> uses a single array with length <span class="math notranslate nohighlight">\(\mathtt{nc} = N_x \times N_\theta\)</span>, and the velocity triplet <span class="math notranslate nohighlight">\((\xi,{\rm v},a)\)</span> uses a single array with length <span class="math notranslate nohighlight">\(\mathtt{nv} = N_\xi \times N_{\rm v} \times N_a\)</span>.  In the binormal direction, <span class="math notranslate nohighlight">\(N_y\)</span> values of <span class="math notranslate nohighlight">\(k_y\)</span> are simulated, with the <span class="math notranslate nohighlight">\(h_a\)</span> for different values of <span class="math notranslate nohighlight">\(k_y\)</span> <em>independent</em> in the linear case.  First, there is a <strong>collisionless layout</strong> for the linear terms in <span class="math notranslate nohighlight">\(A(\Hf,\pf)\)</span> with <code class="docutils literal notranslate"><span class="pre">nc</span></code> configuration space gridpoints on an MPI task, but distributed in velocity space (<code class="docutils literal notranslate"><span class="pre">nv</span></code> gridpoints) on communicator 1 and in <span class="math notranslate nohighlight">\(k_y\)</span> on communicator 2 (with a single <span class="math notranslate nohighlight">\(k_y\)</span> per task):</p>
<div class="math notranslate nohighlight">
\[\mathtt{h(ic,iv\_loc)} \longrightarrow \underbrace{\kx,\theta}_{\mathtt{ic}},[k_y]_2,\underbrace{[\xi,{\rm v},a]_1}_{\mathtt{iv\_loc}} \; .\]</div>
<p>There is no distributed <span class="math notranslate nohighlight">\(k_y\)</span> index because there is <em>one</em> value of <span class="math notranslate nohighlight">\(k_y\)</span> per MPI task.  The <strong>collisional layout</strong> for <span class="math notranslate nohighlight">\(B(H_a,\Psi_a)\)</span> has all of velocity space on an MPI task, but is distributed in configuration space:</p>
<div class="math notranslate nohighlight">
\[\mathtt{h(ic\_loc,iv)} \longrightarrow \underbrace{[\kx,\theta]_1}_{\mathtt{ic\_loc}},[k_y]_2,\underbrace{\xi,{\rm v},a}_{\mathtt{iv}} \; .\]</div>
<p>Finally, there is a <strong>nonlinear layout</strong></p>
<div class="math notranslate nohighlight">
\[\mathtt{h(ir,j\_loc,in)} \longrightarrow \underbrace{\kx}_{\mathtt{ir}},\underbrace{[\theta,[\xi,{\rm v},a]_1]_2}_\mathtt{j\_loc},  \underbrace{k_y}_\mathtt{in} \; .\]</div>
<p>To switch from the collisionless to the collisional layout and back, we use a <em>collision communication</em> (<code class="docutils literal notranslate"><span class="pre">coll_comm</span></code>).  To treat the nonlinearity in <span class="math notranslate nohighlight">\(A(\Hf,\pf)\)</span>, the linear process grid is multiplied by <span class="math notranslate nohighlight">\(N_y\)</span> and all toroidal modes are collected on a single core using the <em>nonlinear communication</em> (<code class="docutils literal notranslate"><span class="pre">nl_comm</span></code>).  These two communication operations use <code class="docutils literal notranslate"><span class="pre">MPI_ALLTOALL</span></code>, but only on a <em>single</em> (not both) MPI subcommunicator. A relatively inexpensive <em>field communication</em> (<code class="docutils literal notranslate"><span class="pre">field_comm</span></code>) based on <code class="docutils literal notranslate"><span class="pre">MPI_ALLREDUCE</span></code> solves the Maxwell equations. Finally, there is a communication associated with the conservative upwind scheme (<code class="docutils literal notranslate"><span class="pre">str_comm</span></code>).  The eight <em>computational kernels</em> are summarized in <a class="reference internal" href="#tab-kernels"><span class="std std-numref">Table 49</span></a>.</p>
</section>
</section>
<section id="parallel-performance-and-scalability">
<h2>Parallel Performance and Scalability<a class="headerlink" href="#parallel-performance-and-scalability" title="Link to this heading"></a></h2>
<section id="strong-scaling-performance">
<h3>Strong-scaling performance<a class="headerlink" href="#strong-scaling-performance" title="Link to this heading"></a></h3>
<p>Part (a) of <a class="reference internal" href="#fig-nl03"><span class="std std-numref">Fig. 1</span></a> shows strong-scaling results for two CPU-only architectures (<em>NERSC Cori</em> (KNL) and <em>TACC Stampede2</em> (Skylake)) and three hybrid-CPU/GPU architectures (<em>OLCF Summit</em>, <em>NERSC Perlmutter</em>, and <em>OLCF Frontier</em>).  For clarity, we restrict ourselves to simple node-based comparisons.  The benchmark test case <code class="docutils literal notranslate"><span class="pre">nl03</span></code> is broadly representative of our targeted simulations at coarser resolution, being somewhere between traditional ion-scale resolution and full multiscale resolution with <span class="math notranslate nohighlight">\((N_x,N_y,N_\theta,N_\xi, N_v, N_a) = (512,128,32,24,8,3)\)</span>. All systems scale well, with Frontier and Perlmutter by far the best performers on both a per-node and maximum performance basis.</p>
<figure class="sphinx-subfigure align-center" id="fig-nl03" style="width: 100%">
<div class="sphinx-subfigure-grid ss-layout-default-AB" style="display: grid;">
<div class="sphinx-subfigure-area" style="display: flex; flex-direction: column; justify-content: center; align-items: center; grid-area: A;">
<img alt="_images/nl03_strong_frontier.png" src="_images/nl03_strong_frontier.png" />
</div>
<div class="sphinx-subfigure-area" style="display: flex; flex-direction: column; justify-content: center; align-items: center; grid-area: B;">
<img alt="_images/nl03_bar_frontier.png" src="_images/nl03_bar_frontier.png" />
</div>
</div>
<figcaption>
<p><span class="caption-number">Fig. 1 </span><span class="caption-text">The (a) Multi-platform strong-scaling comparison for CGYRO test case <code class="docutils literal notranslate"><span class="pre">nl03</span></code>, showing wallclock time vs. number of nodes. Frontier is by far the best performer on both a per-node and maximum performance basis. (b) Kernel-level analysis. Left (darker) bars indicate compute time; right (faded) bars indicate the communication time.  Data is normalized to the total time, such  that total bar area is constant (1.0). Lower compute-to-communication ratio on GPU systems reflects the extremely high performance of the GPUs.  Note the significant improvement in communication management from Summit/Perlmutter to Frontier.</span><a class="headerlink" href="#fig-nl03" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="kernel-based-performance-analysis">
<h3>Kernel-based performance analysis<a class="headerlink" href="#kernel-based-performance-analysis" title="Link to this heading"></a></h3>
<p>Part (b) of <a class="reference internal" href="#fig-nl03"><span class="std std-numref">Fig. 1</span></a> shows a breakdown of the time spent in each computational kernel (see <a class="reference internal" href="#tab-kernels"><span class="std std-numref">Table 49</span></a>). Data for the CPU-only systems is taken at 128 nodes for Cori and 64 nodes for Skylake.  Data for the GPU architectures is taken at 32 nodes. The data is normalized to the total time, ensuring that the total bar area is constant.  On the CPU systems, the compute time is dominated by <code class="docutils literal notranslate"><span class="pre">nl</span></code>.  This is a feature of the spectral algorithm that pushes the computational burden to the nonlinear (FFT) term. On the GPU systems, the <strong>high performance of cuFFT/rocFFT</strong> means a relatively short time spent in <code class="docutils literal notranslate"><span class="pre">nl</span></code>.  This is evident in the small area of the solid blue bar on all GPU systems.  On CPU systems, the time spent in <code class="docutils literal notranslate"><span class="pre">nl</span></code> is higher.  A second apparent feature of the kernel timings is the <em>high cost of the nonlinear communication</em>, <code class="docutils literal notranslate"><span class="pre">nl_comm</span></code>, implemented using <code class="docutils literal notranslate"><span class="pre">MPI_ALLTOALL</span></code> communication outside the FFT library. On the CPU systems, the cost of <code class="docutils literal notranslate"><span class="pre">nl_comm</span></code> is always smaller than the cost of <code class="docutils literal notranslate"><span class="pre">nl</span></code>, whereas on the GPU systems the opposite is true.  Importantly, this result is due to extremely high GPU performance, rather than poor interconnect performance. On the GPU systems, CGYRO heavily leverages GPU-aware MPI, giving a 30-40% reduction in communication timing.  <a class="reference internal" href="#fig-nl03"><span class="std std-numref">Fig. 1</span></a> part (b)  also shows a significant improvement in communication management from Summit/Perlmutter to Frontier, due to optimizations from the porting to Frontier, which are discussed in the next section.</p>
</section>
<section id="parallel-i-o-performance">
<h3>Parallel I/O performance<a class="headerlink" href="#parallel-i-o-performance" title="Link to this heading"></a></h3>
<p>CGYRO output and checkpointing data are in binary format (single and double precision, respectively).  CGYRO I/O implements MPI-IO for parallel/collective-write to single individual files.  In our experience, I/O takes less than 2% of total time in production runs on Frontier. We remark that I/O timings for the Orion filesystem on Frontier were found to be nearly twice as fast as Alpine on Crusher/Summit.</p>
</section>
</section>
<section id="porting-and-optimizing-for-olcf-frontier">
<h2>Porting and Optimizing for OLCF Frontier<a class="headerlink" href="#porting-and-optimizing-for-olcf-frontier" title="Link to this heading"></a></h2>
<p>Here we elaborate on the development work that was undertaken in early 2023 for porting and optimizing CGYRO for Frontier.
From an application perspective, Frontier’s node architecture is very similar to Summit’s: a multicore CPU is connected by high-speed links to multiple GPUs as accelerators. On Summit, each of the two 21-core IBM P9 CPUs is connected to three Nvidia V100 GPUs by NVLink. On Frontier, one 64-core AMD EPYC 7A53 CPU is connected by AMD Infinity Fabric to four AMD Instinct MI250X accelerators. Each of these accelerators consists of two modules, such that an application sees eight GPUs on a Frontier node. Thus, porting CGYRO from NVIDIA GPU-based systems like Summit and Perlmutter to AMD GPU-based systems like Frontier was relatively straightforward, but performance optimization required more fine-tuning, as described next.</p>
<p>CGYRO uses OpenACC directives to offload computational kernels to GPU accelerators.  On Frontier, OpenACC is supported by the HPE Cray Compiling Environment (CCE) Fortran compiler <span id="id12">[<a class="reference internal" href="zreferences.html#id1295" title="Openacc use for cce fortran. https://support.hpe.com/hpesc/public/docDisplay?docId=a00115296en_us&amp;page=OpenACC_Use.html. Accessed: 2022-06-12.">cce</a>]</span>.  Compilation (first on the OLCF testbed system Crusher) was relatively straightforward and required only minimal code changes to CGYRO, mainly related to explicit specification of fields in existing OpenACC directives that were optional for the NVIDIA compiler. For performing FFTs, on Frontier CGYRO calls the <code class="docutils literal notranslate"><span class="pre">hipFFT</span></code> marshaling library, which in turn uses the optimized <code class="docutils literal notranslate"><span class="pre">rocFFT</span></code> library for AMD GPUs <span id="id13">[<a class="reference internal" href="zreferences.html#id1297" title="Hipfft marshalling library. https://hipfft.readthedocs.io/en/rocm-5.1.3. Accessed: 2022-06-12.">hipa</a>, <a class="reference internal" href="zreferences.html#id1300" title="Rocfft documentation. https://rocfft.readthedocs.io/en/rocm-5.1.3. Accessed: 2022-06-12.">roc</a>]</span>. The <code class="docutils literal notranslate"><span class="pre">hipFFT</span></code> library provides exactly the same interface as the cuFFT library to ease porting. We also use AMD <code class="docutils literal notranslate"><span class="pre">hipfort</span></code>, which provides Fortran interfaces for calling HIP libraries <span id="id14">[<a class="reference internal" href="zreferences.html#id1298" title="Hipfort: fortran interface for gpu kernel libraries. https://github.com/ROCmSoftwarePlatform/hipfort. Accessed: 2022-06-12.">hipb</a>]</span>.</p>
<p>In the optimization effort on Frontier, some fine-tuning was needed to improve performance and scalability. Specifically, it was discovered that the CCE compiler was less capable of automatically choosing the ideal parallelization strategy for some loops, compared to the NVIDIA compiler.  Thus, explicitly directing the compiler to use the OpenACC gang vector parallelization was needed for optimal performance.  This was mainly important for the <em>stream kernel</em> and <em>field kernel</em>.  A sequence of optimizations addressing dominant loop operations was also implemented.  In the <em>stream kernel</em>, we reordered loops to remove the need for reductions.  In the <em>shear kernel</em>, we removed an intermediary table, trading memory intensity for compute intensity.  In both cases, the new code was faster on both AMD and NVIDIA GPU-based systems, but the overall impact was significantly larger for the AMD GPUs. Performance gains comparing the original and optimized code are shown in <a class="reference internal" href="#fig-amd1"><span class="std std-numref">Fig. 2</span></a>.  For the <em>nonlinear kernel</em> (FFT), we also improved the zero-padding scheme used to avoid aliasing to provide decompositions that eliminate large primes. Significant speed-ups were then observed on all platforms, but more so on the AMD GPU-based system (factor of 3 for test cases), indicating that NVIDIA libraries are more tolerant of sub-optimal programming patterns.  This is shown in <a class="reference internal" href="#fig-amd2"><span class="std std-numref">Fig. 3</span></a>.  Taken altogether, CGYRO performance on the AMD GPU-nodes on OLCF Frontier is now faster than on the NVIDIA GPU-based nodes on NERSC Perlmutter, as summarized in <a class="reference internal" href="#fig-amd3"><span class="std std-numref">Fig. 4</span></a>.  In addition, with these optimizations for Frontier, modest speed improvements were also seen on NVIDIA GPUs, which was an unexpected benefit.</p>
<p>For the communication, CGYRO heavily leverages GPU-aware MPI on Summit to optimize communication performance. On Frontier, GPU-aware MPI – passing GPU memory addresses directly to MPI routines – is not only supported but is also the recommended way to perform MPI communications, since each of the four HPE Slingshot Network Interface Controller (NIC) is directly connected to the four AMD MI250X GPUs.  In comparison with Perlmutter, we found that the in-node communication capabilities of Frontier are almost identical.  However, we also found that the dedicated NIC-setup in the Frontier system delivers greater performance than the shared-NIC setup in Perlmutter, as shown in Fig.~ref{fig.amd}d.</p>
<p>Below we show CGYRO performance optimizations from porting to Frontier, comparing wallclock benchmark timings (s) of the Frontier AMD GPU-based system with the Perlmutter NVIDIA GPU-based system, both using 24 GPUs.  <em>Original</em> indicates before the Frontier-porting effort. Performance on both systems was improved in all cases.</p>
<figure class="sphinx-subfigure align-center" id="fig-amd1" style="width: 80%">
<div class="sphinx-subfigure-grid ss-layout-default-A" style="display: grid;">
<div class="sphinx-subfigure-area" style="display: flex; flex-direction: column; justify-content: center; align-items: center; grid-area: A;">
<img alt="_images/pearc23_shear.png" src="_images/pearc23_shear.png" />
</div>
</div>
<figcaption>
<p><span class="caption-number">Fig. 2 </span><span class="caption-text">CGYRO performance optimization results from porting to Frontier, comparing original and optimized <em>shear</em> and <em>stream</em> kernels.</span><a class="headerlink" href="#fig-amd1" title="Link to this image"></a></p>
</figcaption>
</figure>
<figure class="sphinx-subfigure align-center" id="fig-amd2" style="width: 80%">
<div class="sphinx-subfigure-grid ss-layout-default-A" style="display: grid;">
<div class="sphinx-subfigure-area" style="display: flex; flex-direction: column; justify-content: center; align-items: center; grid-area: A;">
<img alt="_images/pearc23_nl.png" src="_images/pearc23_nl.png" />
</div>
</div>
<figcaption>
<p><span class="caption-number">Fig. 3 </span><span class="caption-text">CGYRO performance optimization results from porting to Frontier, comparing original and optimized <em>nonlinear</em> FFT kernel.</span><a class="headerlink" href="#fig-amd2" title="Link to this image"></a></p>
</figcaption>
</figure>
<figure class="sphinx-subfigure align-center" id="fig-amd3" style="width: 80%">
<div class="sphinx-subfigure-grid ss-layout-default-A" style="display: grid;">
<div class="sphinx-subfigure-area" style="display: flex; flex-direction: column; justify-content: center; align-items: center; grid-area: A;">
<img alt="_images/pearc23_compute.png" src="_images/pearc23_compute.png" />
</div>
</div>
<figcaption>
<p><span class="caption-number">Fig. 4 </span><span class="caption-text">CGYRO performance optimization results from porting to Frontier, comparing original and optimized <em>overall compute</em> time.</span><a class="headerlink" href="#fig-amd3" title="Link to this image"></a></p>
</figcaption>
</figure>
<figure class="sphinx-subfigure align-center" id="fig-amd4" style="width: 96%">
<div class="sphinx-subfigure-grid ss-layout-default-A" style="display: grid;">
<div class="sphinx-subfigure-area" style="display: flex; flex-direction: column; justify-content: center; align-items: center; grid-area: A;">
<img alt="_images/pearc23_amd_comm_1_2.png" src="_images/pearc23_amd_comm_1_2.png" />
</div>
</div>
<figcaption>
<p><span class="caption-number">Fig. 5 </span><span class="caption-text">CGYRO performance optimization results from porting to Frontier, showing communication benchmark with <em>final</em> code.</span><a class="headerlink" href="#fig-amd4" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="input_gacode.html" class="btn btn-neutral float-left" title="input.gacode" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="workshop.html" class="btn btn-neutral float-right" title="CGYRO Workshop 2022" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2000-2025.
      <span class="lastupdated">Last updated on Jul 07, 2025.
      </span></p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>